{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f7b71f-22f3-4a68-8d50-7b3aaaf73fe8",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Using our Vector DB\n",
    "\n",
    "In section I, we built a Vector DB to allow for retrieval of similar documents.  This direct followup will show how to use the Vector DB to enhance our prompts with additional context before we put it into a Large Language Model.  \n",
    "\n",
    "The notebook follows as:\n",
    "\n",
    "1. RAG Conceptually\n",
    "   - Question-Answering using Large Language Models\n",
    "   - Retrieval of Relevant Documents for a Query\n",
    "   - Question-Answering using RAG for Document Context\n",
    "2. Using built-in LangChain RAG prompts and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72e2ea-4066-41f3-ad65-8cffe5cbbb45",
   "metadata": {},
   "source": [
    "## 1. RAG Conceptually\n",
    "\n",
    "Large Language Models have proven to be very good at general question and answering tasks.  However, a main limitation of many LLMs is that they are generally constrained to the data that they are initially trained on.  Without access to an external data source, LLMs cannot bring in new information, whether this is proprietary domain specific knowledge or just an update on an existing knowledge base.  Given that, how can we enable LLMs to be updated with new information while leveraging the powerful language properties?\n",
    "\n",
    "One solution to this Retrieval Augumented Generation (RAG).  In RAG, we leverage the fact that LLMs can be prompted with additional context data to add additional relevant context to a given query before we pass it into the model.  The old pipeline would be:\n",
    "\n",
    "```\n",
    "Query ------> LLM\n",
    "```\n",
    "\n",
    "which with RAG will be updated to\n",
    "\n",
    "```\n",
    "Query ------> Retrieve Relevant Documents ------> Augmented Query ------> LLM\n",
    "```\n",
    "\n",
    "We will retrieve relevant documents using the knowledge base we built with the Vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd74da-3a1a-4d2d-98a0-1e2cd14d45d6",
   "metadata": {},
   "source": [
    "### Question-Answering using Large Language Models\n",
    "\n",
    "We start by looking at a question answering system that simply asks the LLM a question.  In this case, if the model doesn't already know the answer, then there's not much way to inject that knowledge into the model.  Some models may immediately identify that there's not enough context while other models may go off rails and hallucinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aba12d4-0b6b-4632-886a-22f7b6619698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee6de56-c4d6-49d6-8de3-dae631370833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005249500274658203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 1467,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff143ac4370a4b20974d77e387001b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003543853759765625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "tokenizer.model",
       "rate": null,
       "total": 493443,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a43169e8e94aa193d5b09d86bdf3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0034215450286865234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 1795303,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f81e62d33249aab3eb5184b613109f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0033829212188720703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "special_tokens_map.json",
       "rate": null,
       "total": 72,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e125a39b4141aa828bee2e43d137b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003499269485473633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 571,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c269263e1f04d4a9e4129e751d4265d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003607511520385742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "model.safetensors.index.json",
       "rate": null,
       "total": 25125,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942458890b2f4bf586ebd30c06da3ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003412485122680664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b45d17d43444dfb0debf0d8ca4ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0033121109008789062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "model-00001-of-00002.safetensors",
       "rate": null,
       "total": 9942981696,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4253a75bbcfd4f75a7312da85ec2d487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004480838775634766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "model-00002-of-00002.safetensors",
       "rate": null,
       "total": 4540516344,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf00b18980e4c03be2b2469e74017c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0034072399139404297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f471395cb2e4d13bf88044be4ae4699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003326892852783203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 116,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19553561812c4293a1fc0c1a98058db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THE FIRST TIME YOU RUN THIS, IT MIGHT TAKE A WHILE\n",
    "\n",
    "model_path_or_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    use_flash_attention_2=True,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "def generate(prompt):\n",
    "    \"\"\"Convenience function for generating model output\"\"\"\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True).input_ids.cuda()\n",
    "    \n",
    "    # Generate new tokens based on the prompt, up to max_new_tokens\n",
    "    # Sample aacording to the parameter\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_new_tokens=100, \n",
    "            do_sample=True, \n",
    "            top_p=0.9,\n",
    "            temperature=0.9,\n",
    "            use_cache=True\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df20081-5efd-49f8-8a75-6985d0a553ae",
   "metadata": {},
   "source": [
    "Let's ask it a very general question because the ChatGPT has been trained on a huge amount of data and providing any specifics in the question will likely result in a correct answer.  In this situation, the model can't possibly ground itself because it doesn't know the context - yet it will still answer with something that it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6253ce-3580-42db-8eaa-8421f321738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "    Question: What's the efficacy of NeuroGlyde?\n",
      "\n",
      "    Answer: \n",
      "\n",
      "\n",
      "Generated Response:\n",
      "      NeuroGlyde is a product that claims to improve memory and cognitive function. It is a dietary supplement that contains a blend of herbs, vitamins, and minerals that are believed to support brain health. However, there is limited research on the effectiveness of NeuroGlyde for memory and cognitive function. Some studies have shown that certain ingredients in NeuroGlyde, such as Bacillus coagulans, may improve cognitive function in healthy adults\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input for for tokenization, attach any prompt that should be needed\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "    Question: {query}\n",
    "\n",
    "    Answer: \n",
    "\"\"\"\n",
    "\n",
    "query = \"What's the efficacy of NeuroGlyde?\"\n",
    "prompt = PROMPT_TEMPLATE.format(query = query)\n",
    "\n",
    "res = generate(prompt)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated Response:\\n{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da026b3e-c31d-4714-bf43-50e6178b3293",
   "metadata": {},
   "source": [
    "It doesn't know the context, so let's provide it the context.  Which context should we provide?  The context will be retrieved from our vector databse.\n",
    "\n",
    "We will retrieve the relevant documents to this question, inject it into the prompt, and send that to the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc40c8-bfac-45ca-a3fe-01d4a21873ba",
   "metadata": {},
   "source": [
    "### Retrieval of Relevant Documents for a Query\n",
    "\n",
    "We'll briefly revisit our code to retrieve documents from our previous example.  This Vector DB has already been populated with a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077a54f3-d587-4888-9181-db8f6d8ee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db092bd-47a2-4d5f-939d-97cdaf280331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The connection to the database\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver= \"psycopg2\",\n",
    "    host = \"localhost\",\n",
    "    port = \"5432\",\n",
    "    database = \"postgres\",\n",
    "    user= \"username\",\n",
    "    password=\"password\"\n",
    ")\n",
    "\n",
    "# The embedding function that will be used to store into the database\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Creates the database connection to our existing DB\n",
    "db = PGVector(\n",
    "    connection_string = CONNECTION_STRING,\n",
    "    collection_name = \"embeddings\",\n",
    "    embedding_function = embedding_function,\n",
    "    pre_delete_collection = True, # uncomment this to delete existing database first\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7584fa9-8e09-4dd6-8c04-f3ee56c9ec38",
   "metadata": {},
   "source": [
    "#### (restart database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c322e897-7a06-426c-9df9-0a5dd7c83344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df4a944-d1fa-4655-a38a-cbcd369bcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "import glob\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6e095e-9bf2-4af4-ab4e-802d73d6bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(doc_path: str) -> List[Document]:\n",
    "    \"\"\"Chunk a document into smaller langchain Documents for embedding.\n",
    "\n",
    "    :param doc_path: path to document\n",
    "    :type doc_path: str\n",
    "    :return: List of Document chunks\n",
    "    :rtype: List[Document]\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # split document based on the `\\n\\n` character, quite unintuitive\n",
    "    # https://stackoverflow.com/questions/76633836/what-does-langchain-charactertextsplitters-chunk-size-param-even-do\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    \n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# load the document and split it into chunks\n",
    "doc_chunks = []\n",
    "for doc in glob.glob(\"../../msl-data/*.pdf\"):\n",
    "    doc_chunks += chunk_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae23417-b3ab-49af-a15e-2bb1b8d55d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab1994-ae97-43c9-bd27-36eec1e27541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec270116-16d5-41af-9886-89a5c53cd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PGVector.from_documents(\n",
    "    doc_chunks[:5],\n",
    "    connection_string = CONNECTION_STRING,\n",
    "    collection_name = \"embeddings\",\n",
    "    embedding = embedding_function,\n",
    "    pre_delete_collection = True, # uncomment this to delete existing database first\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f94c6570-2ce5-4fc1-ba05-c23a85759835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.26005329906781316\n",
      "NumPy (Numerical Python) is an open source Python library that's used in almost every field of science and engineering. It's the universal standard for working with numerical data in Python, and it's at the core of the scientific Python and PyData ecosystems.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# query it, note that the score here is a distance metric (lower is more related)\n",
    "# query = \"What's the efficacy of NeuroGlyde?\"\n",
    "query = \"What's the efficacy of Numpy?\"\n",
    "docs_with_scores = db.similarity_search_with_score(query, k = 1)\n",
    "\n",
    "# print results\n",
    "for doc, score in docs_with_scores:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927a8435-055b-4848-8619-de7466e38a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"NumPy (Numerical Python) is an open source Python library that's used in almost every field of science and engineering. It's the universal standard for working with numerical data in Python, and it's at the core of the scientific Python and PyData ecosystems.\", metadata={'source': 'sample_docs/sample_docs-Copy4.txt'}),\n",
       "  0.6104906135505066)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257a0ee-96bb-4729-8c25-5141628ad349",
   "metadata": {},
   "source": [
    "When we query, we get the most relevant document for this query.  Let's create a new prompt that can take this new context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e12fbf-d932-4e61-a382-9c02b9eb0674",
   "metadata": {},
   "source": [
    "### Question-Answering using RAG for Document Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55311ce-5281-42c7-a199-2d316be10d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input for for tokenization, attach any prompt that should be needed\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question using only this context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "query = \"What's the efficacy of NeuroGlyde?\"\n",
    "docs_with_scores = db.similarity_search_with_score(query, k = 1)\n",
    "context_prompt = RAG_PROMPT_TEMPLATE.format(\n",
    "    context = docs_with_scores[0][0].page_content,\n",
    "    query = query\n",
    ")\n",
    "\n",
    "res = generate(context_prompt)\n",
    "\n",
    "print(f\"Prompt:\\n{context_prompt}\\n\")\n",
    "print(f\"Generated Response:\\n{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339ebf9-67f5-42f4-851d-3ef233c0d296",
   "metadata": {},
   "source": [
    "That's it! That's the general concept of Retrieval Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c1977-6931-4399-bb03-51b39d4a16d8",
   "metadata": {},
   "source": [
    "## Using built in LangChain RAG chains instead\n",
    "\n",
    "LangChain contains many built-in methods that have connectivity to Vector Databases and LLMs.  In the example above, we built a custom prompt template and manually retrieved the document, then put it into the chain.  While pretty simple, with LangChain, this can all be pipelined together and more can be done, such as retrieving meta-data and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeba65f-56b7-49a0-a5f4-7e217aa70173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# Turn our db into a retriever\n",
    "retriever = db.as_retriever(search_kwargs = {'k' : 2})\n",
    "\n",
    "# Turn our model into an LLM\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=100)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question using only this context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \n",
    "\"\"\")                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8dbc34-26a8-4719-befe-747dc4e95510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build a chain with multiple documents for RAG\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 2-step chain, first retrieve documents\n",
    "# Then take those documents and store relevant infomration in `document_sources`\n",
    "# Pass the prompt into the document chain\n",
    "rag_chain_with_source = RunnableParallel({\n",
    "    \"documents\": retriever, \n",
    "     \"question\": RunnablePassthrough()\n",
    "}) | {\n",
    "    \"sources\": lambda input: [(doc.page_content, doc.metadata) for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1d7ba-0058-4eed-a21b-9f18f3a6b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rag_chain_with_source.invoke(\"What's the efficacy of Pentatryponal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f8327-9b13-4ee1-bb1b-9dd84eddb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399f80f-7c00-4496-90e8-7d8d05583de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['sources']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
